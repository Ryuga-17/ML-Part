{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-KqvIyxOcYq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSHO4WgNO59z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1aee6c1-7238-4e77-afe4-81d8b539967c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-27e142b34f24>:1: DtypeWarning: Columns (1,2,8,9,14,19,20,172,173) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df=pd.read_csv('ibtracs.ALL.list.v04r01 (1).csv')\n"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv('ibtracs.ALL.list.v04r01 (1).csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mf = df[['ISO_TIME', 'LAT', 'LON', 'STORM_SPEED', 'STORM_DIR']].copy()"
      ],
      "metadata": {
        "id": "jS4s8go8vR8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_cyclone_data(df, task='path'):\n",
        "    \"\"\"\n",
        "    Preprocesses cyclone data for LSTM and ML models.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): Raw cyclone dataset.\n",
        "        task (str): 'path' for trajectory prediction (LSTM), 'speed_dir' for speed/direction prediction (ML).\n",
        "\n",
        "    Returns:\n",
        "        Processed DataFrame (X, y) and scalers (if LSTM).\n",
        "    \"\"\"\n",
        "    df = df.copy()  # Avoid modifying original data\n",
        "\n",
        "    # Convert time column to datetime\n",
        "    df['ISO_TIME'] = pd.to_datetime(df['ISO_TIME'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "\n",
        "    # Set index for resampling\n",
        "    df.set_index('ISO_TIME', inplace=True)\n",
        "\n",
        "    # Handle duplicate timestamps (keep first occurrence)\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "    # Convert numeric columns before resampling\n",
        "    for col in ['STORM_SPEED', 'STORM_DIR', 'LAT', 'LON']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Convert objects to numeric before interpolating\n",
        "    df = df.infer_objects(copy=False)\n",
        "\n",
        "    # Resample data every 6 hours and interpolate missing values\n",
        "    df = df.resample('6h').interpolate()\n",
        "\n",
        "    # Reset index after resampling\n",
        "    df.reset_index(inplace=True)\n",
        "\n",
        "    # Circular encoding for wind direction\n",
        "    df['dir_sin'] = np.sin(np.deg2rad(df['STORM_DIR']))\n",
        "    df['dir_cos'] = np.cos(np.deg2rad(df['STORM_DIR']))\n",
        "\n",
        "    # Interaction terms\n",
        "    df['lat_lon_interaction'] = df['LAT'] * df['LON']\n",
        "    df['speed_lat_interaction'] = df['STORM_SPEED'] * df['LAT']\n",
        "    df['speed_lon_interaction'] = df['STORM_SPEED'] * df['LON']\n",
        "\n",
        "    # Create lag features\n",
        "    df['STORM_SPEED_LAG1'] = df['STORM_SPEED'].shift(1)\n",
        "    df['LAT_LAG'] = df['LAT'].shift(1)\n",
        "    df['LON_LAG'] = df['LON'].shift(1)\n",
        "\n",
        "    # Moving Averages\n",
        "    df['LAT_MA3'] = df['LAT'].rolling(window=3).mean()\n",
        "    df['LON_MA3'] = df['LON'].rolling(window=3).mean()\n",
        "    df['SPEED_MA3'] = df['STORM_SPEED'].rolling(window=3).mean()\n",
        "\n",
        "    # Differences (Change over time)\n",
        "    df['DIST_CHANGE'] = np.sqrt((df['LAT'] - df['LAT_LAG'])**2 + (df['LON'] - df['LON_LAG'])**2)\n",
        "    df['SPEED_CHANGE'] = df['STORM_SPEED'].diff()\n",
        "\n",
        "    # Standard deviation over 3 periods\n",
        "    df['SPEED_STD3'] = df['STORM_SPEED'].rolling(window=3).std()\n",
        "\n",
        "    # Time-based features (Extract **after** resetting index)\n",
        "    df['YEAR'] = df['ISO_TIME'].dt.year\n",
        "    df['MONTH'] = df['ISO_TIME'].dt.month\n",
        "    df['DAY'] = df['ISO_TIME'].dt.day\n",
        "    df['HOUR'] = df['ISO_TIME'].dt.hour\n",
        "\n",
        "    # Fill missing values with forward fill\n",
        "    df.ffill(inplace=True)\n",
        "\n",
        "    # Define feature sets based on task\n",
        "    if task == 'path':\n",
        "        features = ['LAT', 'LON', 'STORM_SPEED', 'HOUR', 'MONTH',\n",
        "                    'lat_lon_interaction', 'speed_lat_interaction', 'speed_lon_interaction',\n",
        "                    'dir_sin', 'dir_cos']\n",
        "        target_cols = ['LAT', 'LON']  # Predicting future position\n",
        "    elif task == 'speed_dir':\n",
        "        features = ['LAT', 'LON', 'HOUR', 'MONTH', 'dir_sin', 'dir_cos',\n",
        "                    'STORM_SPEED_LAG1', 'LAT_LAG', 'LON_LAG',\n",
        "                    'LAT_MA3', 'LON_MA3', 'SPEED_MA3', 'SPEED_STD3']\n",
        "        target_cols = ['STORM_SPEED', 'STORM_DIR']  # Predicting speed and direction\n",
        "    else:\n",
        "        raise ValueError(\"Invalid task type! Choose 'path' or 'speed_dir'.\")\n",
        "\n",
        "    # Drop NaN values after feature creation\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Extract features and target\n",
        "    X = df[features].values\n",
        "    y = df[target_cols].values\n",
        "\n",
        "    # Scaling (for LSTM only)\n",
        "    if task == 'path':\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_y = StandardScaler()\n",
        "        X = scaler_X.fit_transform(X)\n",
        "        y = scaler_y.fit_transform(y)\n",
        "\n",
        "        # Reshape for LSTM [samples, timesteps, features]\n",
        "        X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
        "\n",
        "        return X, y, scaler_X, scaler_y  # Return scalers for inverse transformation\n",
        "\n",
        "    return X, y  # ML models don’t need reshaping or scaling"
      ],
      "metadata": {
        "id": "i0HO_Y5Xx7VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Create models\n",
        "speed_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "dir_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "2_L2fivzfnpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_cyclone_data(df, task='speed_dir')  # X = features, y = [STORM_SPEED, STORM_DIR]\n",
        "\n",
        "# Split into training & test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Extract target variables\n",
        "y_train_speed, y_train_dir = y_train[:, 0], y_train[:, 1]  # Speed & Direction separately\n",
        "y_test_speed, y_test_dir = y_test[:, 0], y_test[:, 1]\n",
        "\n",
        "# Create XGBoost models\n",
        "speed_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "dir_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train models\n",
        "speed_model.fit(X_train, y_train_speed)\n",
        "dir_model.fit(X_train, y_train_dir)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_speed = speed_model.predict(X_test)\n",
        "y_pred_dir = dir_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "print(\"⚡ **Speed Model Performance**\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test_speed, y_pred_speed))\n",
        "print(\"R² Score:\", r2_score(y_test_speed, y_pred_speed))\n",
        "\n",
        "print(\"\\n🧭 **Direction Model Performance**\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test_dir, y_pred_dir))\n",
        "print(\"R² Score:\", r2_score(y_test_dir, y_pred_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgyKzgLSzxW_",
        "outputId": "9a9c044b-e6d2-4b06-c82a-027d3814026c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-5f94d251fc41>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[col] = pd.to_numeric(df[col], errors='coerce')\n",
            "<ipython-input-12-5f94d251fc41>:35: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
            "  df = df.resample('6h').interpolate()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ **Speed Model Performance**\n",
            "MAE: 0.9540082789638384\n",
            "R² Score: 0.93415075550222\n",
            "\n",
            "🧭 **Direction Model Performance**\n",
            "MAE: 0.10304005607331024\n",
            "R² Score: 0.9999960459225579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained models\n",
        "joblib.dump(speed_model, \"speed_model.pkl\")  # Save speed prediction model\n",
        "joblib.dump(dir_model, \"dir_model.pkl\")  # Save direction prediction model\n",
        "\n",
        "print(\"✅ Models saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY72sRhxP7WN",
        "outputId": "f469fd97-3f08-43ac-a565-d2ebf6744a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Models saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def preprocess_cyclone_data_lstm(df):\n",
        "    \"\"\"Preprocess cyclone data for LSTM model.\"\"\"\n",
        "\n",
        "    # Ensure datetime format\n",
        "    df = df.copy()  # Avoid modifying original DataFrame\n",
        "    df['ISO_TIME'] = pd.to_datetime(df['ISO_TIME'], errors='coerce')\n",
        "\n",
        "    # Extract time-based features\n",
        "    df['hour'] = df['ISO_TIME'].dt.hour\n",
        "    df['month'] = df['ISO_TIME'].dt.month\n",
        "\n",
        "    # Convert numeric columns to float\n",
        "    numeric_cols = ['LAT', 'LON', 'STORM_SPEED', 'STORM_DIR']\n",
        "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Drop rows with missing values\n",
        "    df = df.dropna(subset=numeric_cols).copy()\n",
        "\n",
        "    # Convert storm direction to sine & cosine\n",
        "    df.loc[:, 'dir_sin'] = np.sin(np.deg2rad(df['STORM_DIR']))\n",
        "    df.loc[:, 'dir_cos'] = np.cos(np.deg2rad(df['STORM_DIR']))\n",
        "\n",
        "    # Interaction terms\n",
        "    df.loc[:, 'lat_lon_interaction'] = df['LAT'] * df['LON']\n",
        "    df.loc[:, 'speed_lat_interaction'] = df['STORM_SPEED'] * df['LAT']\n",
        "    df.loc[:, 'speed_lon_interaction'] = df['STORM_SPEED'] * df['LON']\n",
        "\n",
        "    # Define features and target\n",
        "    features = [\n",
        "        'LAT', 'LON', 'STORM_SPEED', 'hour', 'month',\n",
        "        'lat_lon_interaction', 'speed_lat_interaction', 'speed_lon_interaction',\n",
        "        'dir_sin', 'dir_cos'\n",
        "    ]\n",
        "    target_cols = ['LAT', 'LON']\n",
        "\n",
        "    # Extract values\n",
        "    X = df[features].values\n",
        "    y = df[target_cols].values\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_scaled = scaler_X.fit_transform(X)\n",
        "    y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "    # Reshape for LSTM input (samples, timesteps=1, features)\n",
        "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "    return X_scaled, y_scaled, scaler_X, scaler_y\n",
        "\n",
        "# Preprocess data for LSTM\n",
        "X_scaled, y_scaled, scaler_X, scaler_y = preprocess_cyclone_data_lstm(df)\n",
        "\n",
        "# Split data for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build LSTM Model\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(2)  # Predicting (LAT, LON)\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Define Early Stopping (optional)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50, batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=1, callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Predict\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Display Sample Predictions\n",
        "y_test_actual = scaler_y.inverse_transform(y_test)\n",
        "for i in range(5):\n",
        "    print(f\"Actual: {y_test_actual[i]}, Predicted: {y_pred[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "xXOxtzHj3Wao",
        "outputId": "17016f1e-dc91-4209-da01-9a72d41a6a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-19b4d5f0bb9a>:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['ISO_TIME'] = pd.to_datetime(df['ISO_TIME'], errors='coerce')\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m17936/17936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 4ms/step - loss: 0.0163 - mae: 0.0309 - val_loss: 7.3949e-05 - val_mae: 0.0060\n",
            "Epoch 2/50\n",
            "\u001b[1m17936/17936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 4ms/step - loss: 3.6736e-05 - mae: 0.0041 - val_loss: 1.6908e-05 - val_mae: 0.0028\n",
            "Epoch 3/50\n",
            "\u001b[1m17936/17936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 4ms/step - loss: 2.6474e-05 - mae: 0.0035 - val_loss: 1.9108e-05 - val_mae: 0.0033\n",
            "Epoch 4/50\n",
            "\u001b[1m17936/17936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 4ms/step - loss: 2.1969e-05 - mae: 0.0032 - val_loss: 7.4142e-06 - val_mae: 0.0019\n",
            "Epoch 5/50\n",
            "\u001b[1m17936/17936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 4ms/step - loss: 1.9582e-05 - mae: 0.0030 - val_loss: 7.0120e-06 - val_mae: 0.0020\n",
            "Epoch 6/50\n",
            "\u001b[1m17936/17936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 4ms/step - loss: 1.5041e-05 - mae: 0.0026 - val_loss: 7.4416e-06 - val_mae: 0.0020\n",
            "Epoch 7/50\n",
            "\u001b[1m17936/17936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 5ms/step - loss: 1.5785e-05 - mae: 0.0025 - val_loss: 8.4089e-06 - val_mae: 0.0021\n",
            "Epoch 8/50\n",
            "\u001b[1m17936/17936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 4ms/step - loss: 1.4082e-05 - mae: 0.0025 - val_loss: 5.1060e-06 - val_mae: 0.0017\n",
            "Epoch 9/50\n",
            "\u001b[1m17936/17936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3ms/step - loss: 1.2816e-05 - mae: 0.0024 - val_loss: 6.8541e-06 - val_mae: 0.0017\n",
            "Epoch 10/50\n",
            "\u001b[1m13371/17936\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 1.1197e-05 - mae: 0.0022"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-19b4d5f0bb9a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0;34m\"\"\"Calls the graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m     bound_args = function_type_utils.canonicalize_function_inputs(\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values, is_pure)\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_pure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m   bound_arguments = bind_function_inputs(\n\u001b[0m\u001b[1;32m    423\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py\u001b[0m in \u001b[0;36mbind_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     bound_arguments = function_type.bind_with_defaults(\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitized_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36mbind_with_defaults\u001b[0;34m(self, args, kwargs, default_values)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Returns BoundArguments with default values filled in.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcan\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \"\"\"\n\u001b[0;32m-> 3195\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbind_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36m_bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3062\u001b[0m         \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3064\u001b[0;31m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3065\u001b[0m         \u001b[0mparameters_ex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3066\u001b[0m         \u001b[0marg_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(scaler_X, \"scaler_X.pkl\")\n",
        "joblib.dump(scaler_y, \"scaler_y.pkl\")\n"
      ],
      "metadata": {
        "id": "8hoO__l9S3Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib  # For saving/loading scalers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def preprocess_cyclone_data_lstm(df, scaler_X=None, scaler_y=None, training=True):\n",
        "    \"\"\"Preprocess cyclone data for LSTM training and inference.\"\"\"\n",
        "\n",
        "    df = df.copy()  # Avoid modifying the original DataFrame\n",
        "    df['ISO_TIME'] = pd.to_datetime(df['ISO_TIME'], errors='coerce')\n",
        "\n",
        "    # Extract time-based features\n",
        "    df['hour'] = df['ISO_TIME'].dt.hour.fillna(0).astype(int)\n",
        "    df['month'] = df['ISO_TIME'].dt.month.fillna(0).astype(int)\n",
        "\n",
        "    # Convert numeric columns to float\n",
        "    numeric_cols = ['LAT', 'LON', 'STORM_SPEED', 'STORM_DIR']\n",
        "    for col in numeric_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Fill missing values with median instead of dropping data\n",
        "    df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "\n",
        "    # Convert storm direction to sine & cosine\n",
        "    df['dir_sin'] = np.sin(np.deg2rad(df['STORM_DIR']))\n",
        "    df['dir_cos'] = np.cos(np.deg2rad(df['STORM_DIR']))\n",
        "\n",
        "    # Interaction terms\n",
        "    df['lat_lon_interaction'] = df['LAT'] * df['LON']\n",
        "    df['speed_lat_interaction'] = df['STORM_SPEED'] * df['LAT']\n",
        "    df['speed_lon_interaction'] = df['STORM_SPEED'] * df['LON']\n",
        "\n",
        "    # Define feature columns\n",
        "    features = [\n",
        "        'LAT', 'LON', 'STORM_SPEED', 'hour', 'month',\n",
        "        'lat_lon_interaction', 'speed_lat_interaction', 'speed_lon_interaction',\n",
        "        'dir_sin', 'dir_cos'\n",
        "    ]\n",
        "    target_cols = ['LAT', 'LON']\n",
        "\n",
        "    # Extract feature values\n",
        "    X = df[features].values\n",
        "    y = df[target_cols].values if training else None\n",
        "\n",
        "    # Load or fit scalers\n",
        "    if training:\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_y = StandardScaler()\n",
        "        X_scaled = scaler_X.fit_transform(X)\n",
        "        y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "        # Save scalers for later inference\n",
        "        joblib.dump(scaler_X, \"scaler_X.pkl\")\n",
        "        joblib.dump(scaler_y, \"scaler_y.pkl\")\n",
        "\n",
        "    else:\n",
        "        scaler_X = joblib.load(\"scaler_X.pkl\")\n",
        "        X_scaled = scaler_X.transform(X)\n",
        "        y_scaled = None\n",
        "\n",
        "    # Reshape for LSTM input (samples, timesteps=1, features)\n",
        "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "    return X_scaled, y_scaled, scaler_X, scaler_y\n",
        "\n",
        "# Load dataset (Ensure `df` is available)\n",
        "X_scaled, y_scaled, scaler_X, scaler_y = preprocess_cyclone_data_lstm(df)\n",
        "\n",
        "# Split data for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build LSTM Model\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(2)  # Predicting (LAT, LON)\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Define Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50, batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=1, callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Save trained model for later inference\n",
        "model.save(\"cyclone_lstm_model.h5\")\n",
        "\n",
        "# Predict\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Display Sample Predictions\n",
        "y_test_actual = scaler_y.inverse_transform(y_test)\n",
        "for i in range(5):\n",
        "    print(f\"Actual: {y_test_actual[i]}, Predicted: {y_pred[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "480Ky1wK5r4i",
        "outputId": "6f53c709-9cef-4887-92b8-b172e009fb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0727 - mae: 0.1120 - val_loss: 1.0720e-04 - val_mae: 0.0076\n",
            "Epoch 2/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 8.4055e-05 - mae: 0.0066 - val_loss: 6.6684e-05 - val_mae: 0.0065\n",
            "Epoch 3/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - loss: 5.0780e-05 - mae: 0.0052 - val_loss: 8.8522e-05 - val_mae: 0.0071\n",
            "Epoch 4/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 4.1799e-05 - mae: 0.0046 - val_loss: 3.0547e-05 - val_mae: 0.0039\n",
            "Epoch 5/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 3.0355e-05 - mae: 0.0040 - val_loss: 2.0953e-05 - val_mae: 0.0038\n",
            "Epoch 6/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 2.2312e-05 - mae: 0.0035 - val_loss: 8.9374e-06 - val_mae: 0.0023\n",
            "Epoch 7/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 2.0813e-05 - mae: 0.0032 - val_loss: 2.0565e-05 - val_mae: 0.0034\n",
            "Epoch 8/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 2.0385e-05 - mae: 0.0032 - val_loss: 2.5539e-05 - val_mae: 0.0040\n",
            "Epoch 9/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 2.0097e-05 - mae: 0.0032 - val_loss: 2.6130e-05 - val_mae: 0.0040\n",
            "Epoch 10/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 2.2340e-05 - mae: 0.0033 - val_loss: 1.3985e-05 - val_mae: 0.0028\n",
            "Epoch 11/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 1.9288e-05 - mae: 0.0031 - val_loss: 1.6827e-05 - val_mae: 0.0031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "Actual: [17.6 87.2], Predicted: [17.537634 86.9107  ]\n",
            "Actual: [18.2 84.6], Predicted: [18.213432 84.32055 ]\n",
            "Actual: [ 11.5 111.7], Predicted: [ 11.43135 111.57744]\n",
            "Actual: [ 25.  -60.4], Predicted: [ 24.967434 -60.16016 ]\n",
            "Actual: [-14.8  76. ], Predicted: [-14.761486  75.89571 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained model and scalers\n",
        "model = load_model(\"cyclone_lstm_model.h5\")\n",
        "scaler_X = joblib.load(\"scaler_X.pkl\")\n",
        "scaler_y = joblib.load(\"scaler_y.pkl\")\n",
        "\n",
        "# Load new cyclone data (`new_df` should be a Pandas DataFrame)\n",
        "X_new, _, _, _ = preprocess_cyclone_data_lstm(new_df, training=False)\n",
        "\n",
        "# Predict\n",
        "y_pred_scaled = model.predict(X_new)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Print predicted locations\n",
        "print(\"Predicted cyclone locations (LAT, LON):\", y_pred)"
      ],
      "metadata": {
        "id": "iAXjN1bo5sUw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "52f35477-0108-4c4b-8129-70ceb57d4774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-85a4754197b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load trained model and scalers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cyclone_lstm_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscaler_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaler_X.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscaler_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaler_y.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ueR9WYYsgoDy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}